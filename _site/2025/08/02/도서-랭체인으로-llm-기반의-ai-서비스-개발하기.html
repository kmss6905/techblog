<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="[Book] 랭체인으로 LLM 기반의 AI 서비스 개발하기" /><meta name="author" content="김민식" /><meta property="og:locale" content="en_US" /><meta name="description" content="서론" /><meta property="og:description" content="서론" /><link rel="canonical" href="http://localhost:4000/2025/08/02/%EB%8F%84%EC%84%9C-%EB%9E%AD%EC%B2%B4%EC%9D%B8%EC%9C%BC%EB%A1%9C-llm-%EA%B8%B0%EB%B0%98%EC%9D%98-ai-%EC%84%9C%EB%B9%84%EC%8A%A4-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0" /><meta property="og:url" content="http://localhost:4000/2025/08/02/%EB%8F%84%EC%84%9C-%EB%9E%AD%EC%B2%B4%EC%9D%B8%EC%9C%BC%EB%A1%9C-llm-%EA%B8%B0%EB%B0%98%EC%9D%98-ai-%EC%84%9C%EB%B9%84%EC%8A%A4-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0" /><meta property="og:site_name" content="김민식 기술블로그" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-08-02T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[Book] 랭체인으로 LLM 기반의 AI 서비스 개발하기" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"김민식","url":"https://minshikkim.com"},"dateModified":"2025-08-02T00:00:00+09:00","datePublished":"2025-08-02T00:00:00+09:00","description":"서론","headline":"[Book] 랭체인으로 LLM 기반의 AI 서비스 개발하기","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/08/02/%EB%8F%84%EC%84%9C-%EB%9E%AD%EC%B2%B4%EC%9D%B8%EC%9C%BC%EB%A1%9C-llm-%EA%B8%B0%EB%B0%98%EC%9D%98-ai-%EC%84%9C%EB%B9%84%EC%8A%A4-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0"},"url":"http://localhost:4000/2025/08/02/%EB%8F%84%EC%84%9C-%EB%9E%AD%EC%B2%B4%EC%9D%B8%EC%9C%BC%EB%A1%9C-llm-%EA%B8%B0%EB%B0%98%EC%9D%98-ai-%EC%84%9C%EB%B9%84%EC%8A%A4-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0"}</script><title> [Book] 랭체인으로 LLM 기반의 AI 서비스 개발하기 - 김민식 기술블로그</title><link rel="shortcut icon" href="/favicon.ico"><link rel="alternate" type="application/atom+xml" title="김민식 기술블로그" href="/atom.xml"><link rel="alternate" type="application/json" title="김민식 기술블로그" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script src="https://cdn.jsdelivr.net/npm/mermaid@11.9.0/dist/mermaid.min.js"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:Inter,SF Pro,Segoe UI,Roboto,Oxygen,Ubuntu,Helvetica Neue,Helvetica,Arial,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.7;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:2.5rem 0 1rem 0}.post ul,.post ol{margin-left:1.5rem}.post table{border:1px solid gray;border-collapse:collapse}.post table td,.post table th{border:1px solid gray;padding:.5rem 1rem}.post table th{white-space:nowrap}.meta{margin:2rem 0}.meta p{margin:0}.meta time{display:block;margin-bottom:.5rem}.highlight code,pre{background:#fafafa}.highlight code{padding:.1rem}h1>code,h2>code,h3>code,h4>code,h5>code,h6>code,li>code,p>code{background-color:rgba(129,139,152,.1215686275);padding:2px 4px;border-radius:6px}pre.mermaid{background:none}pre.mermaid>*{background:inherit}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0;margin:1.5rem 0 2rem}header{flex-basis:10rem;flex-grow:1;position:relative}header>nav:first-child{margin-bottom:2rem}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{border-left:5px solid #ececec;padding-left:1rem;margin:1rem 0}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{font-size:2.125em;font-weight:800;margin-top:0;margin-bottom:.5em;line-height:1.5}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.pagination{margin-top:1.5rem;display:flex;align-items:center;gap:1rem;margin-left:auto}.pagination .page_number{margin-right:1rem}.pagination .page_number .current{font-weight:bold}.pagination img{width:24px;height:24px;display:block;padding:4px}.pagination img.disabled{opacity:.3}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:0 auto 2rem auto;padding:1rem}.title{margin:2rem auto 0 auto;max-width:60rem;padding-left:12rem;font-size:24px}.title a{text-decoration:none;font-weight:bold}.post a{overflow-wrap:break-word}.post li{margin-top:.25rem;margin-bottom:.25rem}.post iframe{max-width:100%}.mobile{display:none}@media screen and (max-width: 45rem){header{display:none}header.active{display:block;position:fixed;left:0;top:0;background:#fff;width:100%;height:100%;padding:2rem}header ul{border-right:none}header li{text-align:left;margin-right:0}header li>a{display:block;padding:.25rem 0}.logo{padding-bottom:1rem}.photos ul{margin-top:.5rem}.title{margin:2rem auto 0;padding:0;text-align:center}.mobile{display:flex}.mobile.menu-button{position:absolute;padding:.25rem;margin-left:1.5rem;cursor:pointer;-webkit-tap-highlight-color:rgba(0,0,0,0)}.mobile.close-button{position:absolute;right:1.5rem;top:2rem;padding:.25rem;cursor:pointer;-webkit-tap-highlight-color:rgba(0,0,0,0)}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}section.post .toc{border:1px solid #ccc;font-size:15px;margin:1rem 0 1.5rem}section.post .toc a{text-decoration:none}section.post .table-wrapper{overflow-x:auto;padding-bottom:1rem}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}.content{--content-heading-weight: var(--weight-extrabold);--content-heading-line-height: 1.125;--content-block-margin-bottom: 1em;--content-blockquote-padding: 0.5em 1.5em;--content-pre-padding: 1.25em 1.5em;--content-table-cell-border-width: 0 0 1px;--content-table-cell-padding: 0.5em 0.75em;--content-table-head-cell-border-width: 0 0 2px;--content-table-body-last-row-cell-border-bottom-width: 0;--content-table-foot-cell-border-width: 2px 0 0}.content li+li{margin-top:.25em}.content p:not(:last-child),.content dl:not(:last-child),.content ol:not(:last-child),.content ul:not(:last-child),.content blockquote:not(:last-child),.content pre:not(:last-child),.content table:not(:last-child){margin-bottom:var(--content-block-margin-bottom)}.content h1,.content h2,.content h3,.content h4,.content h5,.content h6{font-weight:var(--content-heading-weight);line-height:var(--content-heading-line-height)}.content h1{font-size:2em;margin-bottom:.5em}.content h1:not(:first-child){margin-top:1em}.content h2{font-size:1.75em;margin-bottom:.5714em}.content h2:not(:first-child){margin-top:1.1428em}.content h3{font-size:1.5em;margin-bottom:.6666em}.content h3:not(:first-child){margin-top:1.3333em}.content h4{font-size:1.25em;margin-bottom:.8em}.content h5{font-size:1.125em;margin-bottom:.8888em}.content h6{font-size:1em;margin-bottom:1em}.content blockquote{padding:var(--content-blockquote-padding)}.content ol{list-style-position:outside;margin-inline-start:2em}.content ol:not(:first-child){margin-top:1em}.content ol:not([type]){list-style-type:decimal}.content ol:not([type]).is-lower-alpha{list-style-type:lower-alpha}.content ol:not([type]).is-lower-roman{list-style-type:lower-roman}.content ol:not([type]).is-upper-alpha{list-style-type:upper-alpha}.content ol:not([type]).is-upper-roman{list-style-type:upper-roman}.content ul{list-style:disc outside;margin-inline-start:2em}.content ul:not(:first-child){margin-top:1em}.content ul ul{list-style-type:circle;margin-bottom:.25em;margin-top:.25em}.content ul ul ul{list-style-type:square}.content dd{margin-inline-start:2em}.content figure:not([class]){margin-left:2em;margin-right:2em;text-align:center}.content figure:not([class]):not(:first-child){margin-top:2em}.content figure:not([class]):not(:last-child){margin-bottom:2em}.content figure:not([class]) img{display:inline-block}.content figure:not([class]) figcaption{font-style:italic}.content pre{-webkit-overflow-scrolling:touch;overflow-x:auto;padding:var(--content-pre-padding);white-space:pre;word-wrap:normal}.content sup,.content sub{font-size:75%}.content table td,.content table th{border-width:var(--content-table-cell-border-width);padding:var(--content-table-cell-padding);vertical-align:top}.content table th:not([align]){text-align:inherit}.content table thead td,.content table thead th{border-width:var(--content-table-head-cell-border-width)}.content table tfoot td,.content table tfoot th{border-width:var(--content-table-foot-cell-border-width)}.content table tbody tr:last-child td,.content table tbody tr:last-child th{border-bottom-width:var(--content-table-body-last-row-cell-border-bottom-width)}.content .tabs li+li{margin-top:0}.content.is-small{font-size:var(--size-small)}.content.is-normal{font-size:var(--size-normal)}.content.is-medium{font-size:var(--size-medium)}.content.is-large{font-size:var(--size-large)}:root{--weight-light: 300;--weight-normal: 400;--weight-medium: 500;--weight-semibold: 600;--weight-bold: 700;--weight-extrabold: 800}</style><script async src="https://www.googletagmanager.com/gtag/js?id=UA-150864545-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-150864545-1'); </script></head><body><div class="title"> <a href="/">김민식 기술블로그</a></div><main role="main"><header role="banner"><nav role="navigation"><ul><li><a href="/" >Posts</a></li><li><a href="/categories" >Categories</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li></ul></nav></header><section class="post"><h1>[Book] 랭체인으로 LLM 기반의 AI 서비스 개발하기</h1><div class="toc"><ol id="toc" class="section-nav"><li class="toc-entry toc-h2"><a href="#서론">서론</a></li><li class="toc-entry toc-h2"><a href="#rag-란">RAG 란?</a></li><li class="toc-entry toc-h2"><a href="#rag-구현-과정">RAG 구현 과정</a><ol><li class="toc-entry toc-h3"><a href="#1-정보-검색">1. 정보 검색</a></li><li class="toc-entry toc-h3"><a href="#2-텍스트-생성">2. 텍스트 생성</a></li></ol></li><li class="toc-entry toc-h2"><a href="#rag-구현-시-필요한-것">RAG 구현 시 필요한 것</a><ol><li class="toc-entry toc-h3"><a href="#1-데이터">1. 데이터</a></li><li class="toc-entry toc-h3"><a href="#임베딩">임베딩</a></li><li class="toc-entry toc-h3"><a href="#임베딩모델">임베딩모델</a></li><li class="toc-entry toc-h3"><a href="#벡터-데이터-베이스">벡터 데이터 베이스</a></li></ol></li></ol></div><div class="content"><h2 id="서론">서론</h2><p>언어모델로 부터 시작해서 LLM의 개념, 종류, 특징, 한계, 사용시 주의점 등등 을 앞서 살펴보았다. 입맛에 맞게 LLM을 활용하기 위해서는 파인튜닝(Fine Tuning) 이 필요하지만, 현실적인 어려움으로 인해 RAG를 선호한다. 이 장에서는 RAG의 개념부터 활용방법에 대해서 알아본다.</p><h2 id="rag-란">RAG 란?</h2><p>RAG(Retrieval-Augmented Generation)는 영어 그대로 검색증강생성 기술이다.</p><p>쉽게 설명하자면, LLM이 텍스트를 생성할 때 관련 정보를 찾아 보고(retrieval), 그 정보를 활용하여 새로운 텍스트를 만드는 (generation) 기술이라고 할 수 있다.</p><p>가령 RAG를 사용하는 LLM은 특정 질문에 답하기 위해 인터넷에서 정보를 검색하고, 그 정보를 바탕으로 상세하고 정확한 답변을 생성할 수 있다.</p><h2 id="rag-구현-과정">RAG 구현 과정</h2><p>결국 RAG 의 구현은 두 가지 과정으로 나뉘어진다.</p><ol><li><strong>정보 검색(retrieval)</strong></li><li><strong>텍스트 생성(generation)</strong></li></ol><h3 id="1-정보-검색">1. 정보 검색</h3><p>정보 검색의 과정은 다음과 같다. 우리가 구글에 원하는 것을 검색해서 정보를 얻어내는 과정과 유사하다.</p><ol><li>질문입력 - 필요한 정보를 찾기 위해 구글이나 네이버에 질문을 하거나 키워드를 입력 (이때 이를 query 라고 함. )</li><li>검색 - 검색 엔진은 해당 쿼리와 관련된 정보를 데이터 베이스나 인터넷에서 찾는다.</li><li>유사도검색 - 검색 엔진은 쿼리와 데이터 베이스(혹은 인터넷)에 있는 문서들 사시의 유사도를 계산.(키워드 검색<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, 시멘틱 검색<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>)</li><li>랭킹처리 - 검색결과로 찾아낸 문서들 중에서 어떤 것이 질문과 가장 관련이 높은지를 결정.</li></ol><blockquote><p>랭킹처리와 관련해서도 여러가지 방법이 존재한다. (페이지랭크<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, TF-IDF<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, 클릭률<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>)</p></blockquote><h3 id="2-텍스트-생성">2. 텍스트 생성</h3><ol><li>(검색엔진의 경우) 결과반환<blockquote><p>랭킹이 매겨진 문서 리스트를 사용자에게 보여줌.</p></blockquote></li><li>(LLM의 경우) 텍스트 생성<blockquote><p>사용자의 질문과 검색 결과로 텍스트를 생성한다.</p><p>예시) query - 마이클 잭슨의 가장 유명한 노래는? database or internet - Billie Jean</p><p>query + Billie Jean =&gt; LLM(generation) =&gt; answer</p><p>generation(llm) - 마이클 잭슨의 가장 유명한 노래 중 하나는 Billie Jean 이며, bra bra..</p></blockquote></li></ol><p>LLM 은 제공된 정보를 활용하여 구체적이고 정보에 기반한 텍스트를 생성할 수 있음.</p><h2 id="rag-구현-시-필요한-것">RAG 구현 시 필요한 것</h2><h3 id="1-데이터">1. 데이터</h3><p>특별한 형식을 요구하지는 않는다. CSV, JSON, PDF, DB도 가능하다. 단, 중요한 것은 데이터의 형식이 아닌 규범과 규제를 고려해야한다. 특히 사용하려는 데이터에 개인정보가 포함되어 있는지, 상업적 용도로 사용할 경우 해당되는 지를 확인해야한다.</p><p>흭득한 데이터는 두 가지 방법으로 사용가능하다.</p><ol><li>시맨틱검색</li><li>벡터 검색</li></ol><h3 id="임베딩">임베딩</h3><p>결국 모든 데이터를 컴퓨터가 이해할 수 있도록 정보를 숫자(벡터)로 바꿔야한다. 아래의 예시에서 첫번째 숫자는 ‘과일(fruit)’ 을 두번 째 숫자는 ‘기술(technology)’ 을 나타낸다고 했을 때 아래와 같이 변환될 수 있다.</p><ul><li>‘사과’: <code class="language-html highlighter-rouge">[1, 0]</code></li><li>‘바나나’: <code class="language-html highlighter-rouge">[0.9, 0.1]</code></li><li>‘사과’: <code class="language-html highlighter-rouge">[0.1, 0.9]</code></li></ul><h3 id="임베딩모델">임베딩모델</h3><p>임베딩을 위해서는 <strong>임베딩모델</strong><sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> 이 필요하다.</p><p>여러 임베딩 모델이 존재한다.(Word2Vec<sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>, Glove<sup id="fnref:8"><a href="#fn:8" class="footnote" rel="footnote" role="doc-noteref">8</a></sup>, Open AI Embeding Model<sup id="fnref:9"><a href="#fn:9" class="footnote" rel="footnote" role="doc-noteref">9</a></sup>)</p><div class="language-py highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1"># Open AI Embeding Model 사용
</span><span class="kn">from</span> <span class="n">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="sh">"</span><span class="s">sk-xxxx</span><span class="sh">"</span><span class="p">)</span>

<span class="n">document</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">제프리 힌튼</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">교수</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">토론토 대학</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">사임</span><span class="sh">'</span><span class="p">]</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">document</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">'</span><span class="s">text-embedding-ada-002</span><span class="sh">'</span>
<span class="p">)</span>
<span class="n">response</span>
</code></pre></div></div><p>실행결과</p><div class="language-html highlighter-rouge"><div class="highlight"><pre class="syntax"><code>
CreateEmbeddingResponse(data=[Embedding(embedding=[-0.025331685319542885, -0.019289137795567513, -0.007674173451960087, -0.016426879912614822, -0.008012942969799042, 0.028055671602487564, -0.005883533041924238, 0.004137831274420023, -0.002872629789635539, 0.006886015180498362, ...
</code></pre></div></div><h3 id="벡터-데이터-베이스">벡터 데이터 베이스</h3><p>벡터 데이터 베이스는 말 그대로 벡터를 저장하는 저장소.</p><p>벡터 데이터베이스는 단순히 벡터를 저장하는 것 외에도 데이터를 관리하며, 검색하는 일도 한다.</p><p>일반 데이터베이스와 달리 벡터 테이터베이스는 데이터의 정확한 값 대신 데이터 간의 ‘유사성’을 바탕으로 검색하는 데 사용됩니다.</p><p>일반적인 데이터베이스에서 원하는 데이터를 검색할 때</p><div class="language-sql highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">customers</span> <span class="k">WHERE</span> <span class="n">age</span> <span class="o">&gt;=</span> <span class="mi">30</span><span class="p">;</span>
</code></pre></div></div><p>벡터 데이터베이스에서 검색할 때</p><p>사용자가 “배송 상태를 어떻게 확인할 수 있죠?” 라고 질문을 하게 되면 백터 데이터베이스에 질의를 해야하기 때문에 먼저 <strong>임베딩 처리 과정</strong> 을 거쳐 <strong>벡터</strong> 로 변환해야한다. 이후 아래와 같이 질의를 할 수 있다.</p><div class="language-text highlighter-rouge"><div class="highlight"><pre class="syntax"><code>POST /search
{
    "query_vector": [0.13, -0.24, 0.33, ..., 0.78],
    "top_k": 5
}
</code></pre></div></div><div class="mermaid"> graph TD contents(contents) application-query(application) model(embedding model) embedding(embedding) db[(Vector Database)] contents --&gt; model application-query --&gt; model model --&gt; embedding embedding --&gt; db</div><p>저장된 문서벡터들과 질문 벡터간의 유사성(이때 코사인 유사도, 질문 벡터 간의 유사성)을 계산하여, 검사 결과에 따라 점수가 가장 높은 (상위 랭크) 상위 N개의 벡터들이 반환되고 사용자에게 보여준다.</p><div class="footnotes" role="doc-endnotes"><ol><li id="fn:4"><p><strong>키워드 검색:</strong> 사용자가 입력한 특정 단어나 구문이 포함된 문서를 찾는 가장 일반적인 검색 방식입니다. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li><li id="fn:5"><p><strong>시멘틱 검색(Semantic Search):</strong> 검색어의 의미와 맥락, 사용자의 의도까지 파악하여 관련성 높은 정보를 제공하는 검색 기술입니다. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li><li id="fn:1"><p><strong>페이지랭크(PageRank):</strong> 웹페이지의 상대적 중요도를 결정하는 알고리즘. 중요한 페이지로부터 많은 링크를 받을수록 중요도가 높다고 평가합니다. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li><li id="fn:2"><p><strong>TF-IDF(Term Frequency-Inverse Document Frequency):</strong> 문서 집합에서 특정 단어가 특정 문서 내에서 얼마나 중요한지를 나타내는 통계적 수치. 단어의 빈도와 역문서 빈도를 곱하여 계산합니다. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li><li id="fn:3"><p><strong>클릭률(CTR, Click-Through Rate):</strong> 광고나 링크가 노출된 횟수 대비 클릭된 횟수의 비율. 사용자의 관심도를 측정하는 주요 지표입니다. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li><li id="fn:6"><p><strong>임베딩 모델(Embedding Model):</strong> 텍스트, 이미지 등의 데이터를 컴퓨터가 이해할 수 있는 숫자 벡터로 변환하는 모델. 데이터의 의미적, 문법적 특징을 벡터 공간에 표현합니다. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li><li id="fn:7"><p><strong>Word2Vec:</strong> “비슷한 문맥에서 등장하는 단어는 비슷한 의미를 가진다”는 분포 가설을 기반으로 단어를 벡터로 변환하는 모델입니다. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li><li id="fn:8"><p><strong>GloVe(Global Vectors for Word Representation):</strong> 말뭉치 전체의 통계 정보를 활용하여 단어의 의미를 벡터 공간에 표현하는 모델입니다. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li><li id="fn:9"><p><strong>OpenAI Embedding Model:</strong> 텍스트를 의미와 문맥을 이해하는 숫자 벡터로 변환하는 OpenAI의 모델입니다. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li></ol></div></div><div class="meta"> <time datetime="2025-08-02T00:00:00+09:00">August 2, 2025</time><p> categories: <a href="/category/AI">AI</a></p><p> tags: <a href="/tag/LLM">LLM</a> , <a href="/tag/LangChain">LangChain</a> , <a href="/tag/RAG">RAG</a> , <a href="/tag/Book">Book</a></p></div><script> mermaid.initialize({startOnLoad:true}); window.mermaid.init(undefined, document.querySelectorAll('.language-mermaid')); </script></section></main></body></html>
